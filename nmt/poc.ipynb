{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"cnn_dailymail\"\n",
    "CONFIG = \"3.0.0\"\n",
    "\n",
    "NMT_MODEL_NAME = \"Helsinki-NLP/opus-mt-en-mul\"\n",
    "NMT_MODEL_CONFIG = {}\n",
    "\n",
    "TRANSLATION_PREFIX = \">>nob<< \"\n",
    "\n",
    "SPLIT_PATTERN = \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List, Dict\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/navjo/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "100%|██████████| 3/3 [00:00<00:00,  6.16it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_NAME, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(NMT_MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(NMT_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_input(inp: str) -> List[str]:\n",
    "    sentence_splits = re.split(SPLIT_PATTERN, inp)\n",
    "\n",
    "    #Pre-processing\n",
    "\n",
    "    # Append >>nob<< token\n",
    "    sentence_splits = [TRANSLATION_PREFIX + sentence for sentence in sentence_splits]\n",
    "\n",
    "    return sentence_splits\n",
    "\n",
    "def translate_batch(inp: str) -> List[str]:\n",
    "    split_input = prepare_input(inp)\n",
    "\n",
    "    input_ids = tokenizer(split_input, return_tensors=\"pt\", padding=True).input_ids\n",
    "\n",
    "    outputs = model.generate(input_ids)\n",
    "\n",
    "    translated_input = \"\"\n",
    "    for translated_sentence in outputs:\n",
    "        translated_input += \" \" + tokenizer.decode(translated_sentence, skip_special_tokens=True)\n",
    "\n",
    "    return translated_input\n",
    "\n",
    "def translate_sample(inp: Dict) -> Dict:\n",
    "    translated_article = translate_batch(inp[\"article\"])\n",
    "    translated_highlights = translate_batch(inp[\"highlights\"])\n",
    "\n",
    "    return {\"article\": translated_article, \"highlights\": translated_highlights, \"id\": inp[\"id\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = translate_sample(dataset[\"train\"][10])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('master')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b985b9d2dd14f9c335f7388b3d5393a15b2d7cd2ca2bed1fb46a78eef239c32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
