{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration jkorsvik--cnn_dailymail_nor_v2-0e98d8068d73866c\n",
      "Found cached dataset parquet (C:/Users/jorgen/.cache/huggingface/datasets/jkorsvik___parquet/jkorsvik--cnn_dailymail_nor_v2-0e98d8068d73866c/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 3/3 [00:00<00:00, 90.90it/s]\n",
      "Transform <function _concatenate_map_style_datasets at 0x0000023A6E2ED4C0> couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"jkorsvik/cnn_dailymail_nor_v2\")\n",
    "ds = concatenate_datasets([ds[\"train\"], ds[\"test\"], ds[\"validation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = ds.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "train_val = train_test[\"train\"].train_test_split(test_size=0.07, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = DatasetDict({\n",
    "    \"train\": train_val[\"train\"],\n",
    "    \"validation\": train_val[\"test\"],\n",
    "    \"test\": train_test[\"test\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 47171\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 3551\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 5636\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 48/48 [00:02<00:00, 16.89ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:12<00:00, 12.51s/it]\n",
      "Pushing split validation to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 19.17ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "Pushing split test to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 15.13ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:06<00:00,  6.37s/it]\n"
     ]
    }
   ],
   "source": [
    "full_ds.push_to_hub(\"navjordj/cnn_daily_mail_nor_final\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b985b9d2dd14f9c335f7388b3d5393a15b2d7cd2ca2bed1fb46a78eef239c32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
